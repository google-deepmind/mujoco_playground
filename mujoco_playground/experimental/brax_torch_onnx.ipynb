{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e04a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Device: [CudaDevice(id=0), CudaDevice(id=1)]\n",
      "jax.devices(): [CudaDevice(id=0), CudaDevice(id=1)]\n",
      "local_device_count: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:Created `ArrayHandler` with primary_host=0, replica_id=0, use_replica_parallel=True, array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x7fd0c42e3890>\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.base_pytree_checkpoint_handler.BasePyTreeCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.base_pytree_checkpoint_handler.BasePyTreeCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.array_checkpoint_handler.ArrayCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.array_checkpoint_handler.ArrayCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.proto_checkpoint_handler.ProtoCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.proto_checkpoint_handler.ProtoCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.standard_checkpoint_handler.StandardCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.standard_checkpoint_handler.StandardCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.random_key_checkpoint_handler.JaxRandomKeyCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.random_key_checkpoint_handler.JaxRandomKeyCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint._src.handlers.random_key_checkpoint_handler.NumpyRandomKeyCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint._src.handlers.random_key_checkpoint_handler.NumpyRandomKeyCheckpointHandler'>. Skipping registration.\n",
      "DEBUG:absl:Handler \"orbax.checkpoint.test_utils.ErrorCheckpointHandler\" already exists in the registry with associated type <class 'orbax.checkpoint.test_utils.ErrorCheckpointHandler'>. Skipping registration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\" # 0.9 causes too much lag. \n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "os.environ['JAX_LOG_COMPILES'] = '0'\n",
    "\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "import jax\n",
    "print(\"JAX Device:\", jax.devices())\n",
    "from jax import config # Analytical gradients work much better with double precision.\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update('jax_default_matmul_precision', 'high')\n",
    "\n",
    "print(\"jax.devices():\", jax.devices())\n",
    "print(\"local_device_count:\", jax.local_device_count())\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.DEBUG)\n",
    "\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "\n",
    "# from brax.training.agents.apg import train as apg\n",
    "from apg_alg.algorithm import apg  # Local modified APG version # type: ignore\n",
    "from brax.training.agents.apg import networks as apg_networks\n",
    "\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "\n",
    "from brax.envs.wrappers import training as brax_training\n",
    "\n",
    "from brax.training import acting\n",
    "\n",
    "from brax.io import model\n",
    "\n",
    "from brax import envs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, clear_output\n",
    "from datetime import datetime\n",
    "import mediapy as media\n",
    "\n",
    "import wandb\n",
    "\n",
    "env_name = \"Go2Trot\"\n",
    "env_cfg = registry.get_default_config(env_name)\n",
    "randomizer = registry.get_domain_randomizer(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd8e5e8",
   "metadata": {},
   "source": [
    "# Load environment and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f4f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cfg = registry.get_default_config(env_name)\n",
    "demo_cfg['env']['reset2ref'] = False\n",
    "demo_cfg['env']['reference_state_init'] = False\n",
    "demo_cfg['pert_config']['enable'] = False\n",
    "demo_env = registry.load(env_name, demo_cfg)\n",
    "demo_env = brax_training.VmapWrapper(demo_env)\n",
    "\n",
    "apg_params = locomotion_params.brax_apg_config(env_name)\n",
    "params = model.load_params('/tmp/trotting_apg_2hz_policy')\n",
    "params = (params[0], params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd274526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_size: 40\n",
      "action_size: 12\n",
      "(40,) (40,)\n"
     ]
    }
   ],
   "source": [
    "print(\"obs_size:\", demo_env.observation_size)\n",
    "print(\"action_size:\", demo_env.action_size)\n",
    "params = (params[0], params[1])\n",
    "print(params[0].mean.shape, params[0].std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f493b5a",
   "metadata": {},
   "source": [
    "# Brax to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6721f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# üß© 1. Go2Policy ÂÆö‰πâ\n",
    "# ================================================================\n",
    "class Go2Policy(nn.Module):\n",
    "    def __init__(self, obs_dim=40, act_dim=12, obs_mean=None, obs_std=None, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # Ê≥®ÂÜåÂΩí‰∏ÄÂåñÂèÇÊï∞\n",
    "        if obs_mean is not None and obs_std is not None:\n",
    "            self.register_buffer(\"obs_mean\", torch.tensor(obs_mean, dtype=torch.float32))\n",
    "            self.register_buffer(\"obs_std\", torch.tensor(obs_std, dtype=torch.float32))\n",
    "        else:\n",
    "            self.register_buffer(\"obs_mean\", torch.zeros(obs_dim))\n",
    "            self.register_buffer(\"obs_std\", torch.ones(obs_dim))\n",
    "\n",
    "        # ÁΩëÁªúÁªìÊûÑÔºöLinear ‚Üí ELU ‚Üí LayerNorm ‚Üí Linear ‚Üí ELU ‚Üí LayerNorm ‚Üí Linear\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(256, eps=1e-6),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(128, eps=1e-6),\n",
    "            nn.Linear(128, act_dim * 2),\n",
    "        )\n",
    "\n",
    "    # Ê®°‰ªø running_statistics.normalize\n",
    "    def normalize_obs(self, x):\n",
    "        return (x - self.obs_mean) / (self.obs_std + self.eps)\n",
    "\n",
    "    # ÂâçÂêë‰º†Êí≠\n",
    "    def forward(self, x):\n",
    "        x = self.normalize_obs(x)\n",
    "        out = self.net(x)\n",
    "\n",
    "        mean, log_std = out.chunk(2, dim=-1)\n",
    "        std = torch.exp(log_std)\n",
    "        mean_tanh = torch.tanh(mean)\n",
    "        return mean_tanh, std\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è 2. ÊùÉÈáçÂä†ËΩΩÔºöJAX ‚Üí PyTorch\n",
    "# ================================================================\n",
    "def load_jax_to_torch(jax_params, torch_model):\n",
    "    with torch.no_grad():\n",
    "        # hidden_0\n",
    "        torch_model.net[0].weight.data = torch.tensor(\n",
    "            np.array(jax_params[\"hidden_0\"][\"kernel\"]).T, dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[0].bias.data = torch.tensor(\n",
    "            np.array(jax_params[\"hidden_0\"][\"bias\"]), dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[2].weight.data = torch.tensor(\n",
    "            np.array(jax_params[\"LayerNorm_0\"][\"scale\"]), dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[2].bias.data = torch.tensor(\n",
    "            np.array(jax_params[\"LayerNorm_0\"][\"bias\"]), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # hidden_1\n",
    "        torch_model.net[3].weight.data = torch.tensor(\n",
    "            np.array(jax_params[\"hidden_1\"][\"kernel\"]).T, dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[3].bias.data = torch.tensor(\n",
    "            np.array(jax_params[\"hidden_1\"][\"bias\"]), dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[5].weight.data = torch.tensor(\n",
    "            np.array(jax_params[\"LayerNorm_1\"][\"scale\"]), dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[5].bias.data = torch.tensor(\n",
    "            np.array(jax_params[\"LayerNorm_1\"][\"bias\"]), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # hidden_2\n",
    "        torch_model.net[6].weight.data = torch.tensor(\n",
    "            np.array(jax_params[\"hidden_2\"][\"kernel\"]).T, dtype=torch.float32\n",
    "        )\n",
    "        torch_model.net[6].bias.data = torch.tensor(\n",
    "            np.array(jax_params[\"hidden_2\"][\"bias\"]), dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    print(\"‚úÖ JAX ÊùÉÈáçÂ∑≤ÊàêÂäüÂ§çÂà∂Âà∞ PyTorch Sequential Ê®°ÂûãÔºÅ\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# üîç 3. ÂàÜÂ±ÇÊØîËæÉÔºöÁ≤æÁ°ÆÊü•ÁúãÂì™‰∏ÄÂ±ÇÂºÄÂßã‰∏çÂêå\n",
    "# ================================================================\n",
    "def compare_layers(jax_params, torch_model, obs):\n",
    "    \"\"\"\n",
    "    ÂàÜÂ±ÇÊØîËæÉ JAX ‰∏é PyTorch ÁöÑËæìÂá∫ÔºåÈÄêÂ±ÇÊâìÂç∞Â∑ÆÂºÇ\n",
    "    \"\"\"\n",
    "    x_jax = obs\n",
    "    x_torch = torch.tensor(np.array(obs), dtype=torch.float32)\n",
    "\n",
    "    # ÂèñÂá∫ JAX ÁöÑÊùÉÈáçÂ±ÇÔºàÊåâÈ°∫Â∫èÔºâ\n",
    "    jax_layers = [\n",
    "        jax_params[\"hidden_0\"],\n",
    "        jax_params[\"hidden_1\"],\n",
    "        jax_params[\"hidden_2\"],\n",
    "    ]\n",
    "\n",
    "    layer_idx = 0\n",
    "    for i, module in enumerate(torch_model.net):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # ÂØπÂ∫î JAX Dense\n",
    "            W = np.array(jax_layers[layer_idx][\"kernel\"])\n",
    "            b = np.array(jax_layers[layer_idx][\"bias\"])\n",
    "            x_jax = jnp.dot(x_jax, W) + b\n",
    "            x_torch = module(x_torch)\n",
    "            diff = np.max(np.abs(np.array(x_jax) - x_torch.detach().numpy()))\n",
    "            print(f\"[Linear {layer_idx}] max diff = {diff:.6f}\")\n",
    "            layer_idx += 1\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # ÂØπÂ∫î LayerNorm\n",
    "            scale = np.array(jax_params[f\"LayerNorm_{layer_idx-1}\"][\"scale\"])\n",
    "            bias = np.array(jax_params[f\"LayerNorm_{layer_idx-1}\"][\"bias\"])\n",
    "\n",
    "            mean = np.mean(np.array(x_jax), axis=-1, keepdims=True)\n",
    "            var = np.var(np.array(x_jax), axis=-1, keepdims=True)\n",
    "            x_jax = (np.array(x_jax) - mean) / np.sqrt(var + 1e-6)\n",
    "            x_jax = scale * x_jax + bias\n",
    "\n",
    "            x_torch = module(x_torch)\n",
    "            diff = np.max(np.abs(np.array(x_jax) - x_torch.detach().numpy()))\n",
    "            print(f\"[LayerNorm {layer_idx-1}] max diff = {diff:.6f}\")\n",
    "\n",
    "        elif isinstance(module, nn.ELU):\n",
    "            x_jax = linen.elu(x_jax)\n",
    "            x_torch = module(x_torch)\n",
    "            diff = np.max(np.abs(np.array(x_jax) - x_torch.detach().numpy()))\n",
    "            print(f\"[ELU {layer_idx-1}] max diff = {diff:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b2b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JAX ÊùÉÈáçÂ∑≤ÊàêÂäüÂ§çÂà∂Âà∞ PyTorch Sequential Ê®°ÂûãÔºÅ\n"
     ]
    }
   ],
   "source": [
    "obs_mean = np.array(params[0].mean)\n",
    "obs_std = np.array(params[0].std)\n",
    "torch_model = Go2Policy(\n",
    "    obs_dim=demo_env.observation_size, \n",
    "    act_dim=demo_env.action_size,\n",
    "    obs_mean=obs_mean,\n",
    "    obs_std=obs_std\n",
    ")\n",
    "load_jax_to_torch(params[1]['params'], torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716b73a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f14f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_factory = apg_networks.make_apg_networks\n",
    "apg_training_params = dict(apg_params)\n",
    "if \"network_factory\" in apg_params:\n",
    "    del apg_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        apg_networks.make_apg_networks, \n",
    "        **apg_params.network_factory)\n",
    "# from brax.training.acme import running_statistics\n",
    "normalize = lambda x, y: x\n",
    "\n",
    "def make_normalize_fn(mean, std, max_abs_value=None):\n",
    "    def normalize_fn(batch, _unused_processor_params=None):\n",
    "        def normalize_leaf(data, m, s):\n",
    "            if not jp.issubdtype(data.dtype, jp.inexact):\n",
    "                return data\n",
    "            data = (data - m) / s\n",
    "            if max_abs_value is not None:\n",
    "                data = jp.clip(data, -max_abs_value, +max_abs_value)\n",
    "            return data\n",
    "        return jax.tree_util.tree_map(normalize_leaf, batch, mean, std)\n",
    "    return normalize_fn\n",
    "\n",
    "if apg_params['normalize_observations']:\n",
    "    # normalize = running_statistics.normalize\n",
    "    mean, std = params[0].mean, params[0].std\n",
    "    normalize = make_normalize_fn(mean, std)\n",
    "apg_network = network_factory(\n",
    "    demo_env.observation_size, demo_env.action_size, preprocess_observations_fn=normalize\n",
    ")\n",
    "make_inference_fn = apg_networks.make_inference_fn(apg_network)\n",
    "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddf3575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of observations: <class 'jaxlib._jax.ArrayImpl'>\n",
      "obs shape: (1, 40)\n"
     ]
    }
   ],
   "source": [
    "demo_reset_fn = jax.jit(demo_env.reset)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rngs = jax.random.split(rng, 1) \n",
    "state = demo_reset_fn(rngs)\n",
    "obs = state.obs\n",
    "actions, _ = jit_inference_fn(obs, rng)\n",
    "print(\"type of observations:\", type(obs))\n",
    "print(\"obs shape:\", obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607f8496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX actions: [[-0.00673548  0.06204866 -0.21067517  0.102987    0.03954133 -0.17692343\n",
      "  -0.10118392  0.03572394 -0.10818056 -0.02216607  0.07562327 -0.15920609]]\n",
      "Torch actions: [[-0.00673548  0.06204866 -0.21067518  0.10298702  0.03954135 -0.17692345\n",
      "  -0.10118396  0.03572395 -0.10818054 -0.02216607  0.07562327 -0.15920605]]\n",
      "Max difference: 4.233897325789382e-08\n"
     ]
    }
   ],
   "source": [
    "obs_torch = torch.tensor(np.array(obs), dtype=torch.float32)\n",
    "torch_actions, torch_std = torch_model(obs_torch)\n",
    "torch_actions = torch_actions.detach().numpy()\n",
    "\n",
    "jax_actions_np = np.array(actions)\n",
    "\n",
    "diff = np.abs(jax_actions_np - torch_actions)\n",
    "\n",
    "print(\"JAX actions:\", jax_actions_np)\n",
    "print(\"Torch actions:\", torch_actions)\n",
    "print(\"Max difference:\", diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ce0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear 0] max diff = 0.000000\n",
      "[ELU 0] max diff = 0.000000\n",
      "[LayerNorm 0] max diff = 0.000000\n",
      "[Linear 1] max diff = 0.000000\n",
      "[ELU 1] max diff = 0.000000\n",
      "[LayerNorm 1] max diff = 0.000001\n",
      "[Linear 2] max diff = 0.000001\n"
     ]
    }
   ],
   "source": [
    "# Â±ÇÁ∫ßÂØπÊØî\n",
    "compare_layers(params[1]['params'], torch_model, obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096e63f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249337/1211824759.py:5: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "torch_model.eval()\n",
    "torch.onnx.export(\n",
    "    torch_model,\n",
    "    obs_torch,\n",
    "    \"/home/yxma/develop/mujoco_envs/mujoco_playground/mujoco_playground/experimental/sim2sim/onnx/go2_apg_policy.onnx\",\n",
    "    input_names=[\"obs\"],\n",
    "    output_names=[\"actions\", \"std\"],\n",
    "    dynamic_axes={\n",
    "        \"obs\": {0: \"batch_size\"},\n",
    "        \"actions\": {0: \"batch_size\"},\n",
    "        \"std\": {0: \"batch_size\"},\n",
    "    },\n",
    "    opset_version=17,\n",
    "    dynamo=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
