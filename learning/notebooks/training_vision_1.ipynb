{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ace56453",
      "metadata": {},
      "source": [
        "![MuJoCo banner](https://raw.githubusercontent.com/google-deepmind/mujoco/main/banner.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Copyright notice\n",
        "\n",
        "> <p><small><small>Copyright 2025 DeepMind Technologies Limited.</small></p>\n",
        "> <p><small><small>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a>.</small></small></p>\n",
        "> <p><small><small>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</small></small></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25844711",
      "metadata": {},
      "source": [
        "# Tutorial\n",
        "\n",
        "This notebook is the first of two that demonstrate pixels-to-actions training using the experimental Madrona rendering backend. \n",
        "\n",
        "### Usage\n",
        "\n",
        "Please note that this Colab **does not support a hosted runtime**. To use it, please install [Madrona-MJX](https://github.com/shacklettbp/madrona_mjx/tree/main) and Playground on your local device and select `Connect to a local runtime`, or download the notebook to run locally! We recommend a device with at least 24 GB VRAM, such as the RTX4090."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92bcc22b",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# @title Import MuJoCo, MJX, and Brax\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "\n",
        "from brax.training.agents.ppo import networks_vision as ppo_networks_vision\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from IPython.display import clear_output\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "\n",
        "from mujoco_playground import wrapper\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cda76c2",
      "metadata": {},
      "source": [
        "## Introducing Madrona MJX\n",
        "\n",
        "[Madrona MJX](https://github.com/shacklettbp/madrona_mjx/tree/main) is Mujoco's experimental on-device rendering backend, developed in collaboration with [Madrona](https://github.com/shacklettbp/madrona). It implements a JAX-compatible batched renderer for rolling out multiple training environments in parallel.\n",
        "\n",
        "*The Rendering Bottleneck*\n",
        "\n",
        "In Reinforcement Learning, an agent uses a policy to produce an action $a_t$ in response to an observation $o_t$ of the simulation state. The environment evaluates the interaction via a reward signal $r_t$, then transitions to the next state rendered as $o_{t+1}$. Together, this forms the [transition](https://github.com/google/brax/blob/69637a359463738140c1b850f61ad0088a23538b/brax/training/types.py#L43) $(o_t, a_t, r_t, o_{t+1})$, the basic data unit of a RL training pipeline. Due to poor sample efficiency, the speed of collecting transitions is the often the bottle-neck in on-policy RL methods such as PPO. In addition to full-body physics simulation - for example `mujoco.mjx.step` - pixels-based training requires an expensive call to a renderer on every transition. Along with the cost of expensive CNN architectures, rendering speed becomes a significant performance consideration.\n",
        "\n",
        "*Coming Up*\n",
        "\n",
        "In this tutorial, we'll use a high-performance ray-tracing backend to render images in parallel across a batch of environments. Before we get to sub-minute pixel-based training, let's see how much faster Madrona's batched rendering lets us collect raw transitions. For a baseline, let's load the Cartpole Task from dm_control, a popular reference library built on Mujoco:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0902bd0d",
      "metadata": {},
      "source": [
        "Now, let's load Mujoco Playground's JAX Cartpole re-implementation. While we're at it, we'll configure it for the upcoming training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743928b0",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from mujoco_playground import dm_control_suite\n",
        "\n",
        "N = 1000\n",
        "img_shape = (64, 64)\n",
        "num_envs = 1024\n",
        "ctrl_dt = 0.04\n",
        "episode_length = int(3 / ctrl_dt)\n",
        "\n",
        "config_overrides = {\n",
        "    \"vision\": True,\n",
        "    \"vision_config.render_batch_size\": num_envs,\n",
        "    \"action_repeat\": 1,\n",
        "    \"ctrl_dt\": ctrl_dt,\n",
        "    \"episode_length\": episode_length,\n",
        "}\n",
        "\n",
        "env_name = \"CartpoleBalance\"\n",
        "env = dm_control_suite.load(\n",
        "    env_name, config_overrides=config_overrides\n",
        ")\n",
        "\n",
        "env = wrapper.wrap_for_brax_training(\n",
        "    env,\n",
        "    vision=True,\n",
        "    num_vision_envs=num_envs,\n",
        "    action_repeat=1,\n",
        "    episode_length=episode_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d70a08",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d16dd6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "key_reset, key_act = jax.random.split(jax.random.PRNGKey(0))\n",
        "state = jit_reset(jax.random.split(key_reset, num_envs))\n",
        "\n",
        "# Pre-compile\n",
        "jit_step = jit_step.lower(\n",
        "    state, jp.zeros((num_envs, env.action_size))\n",
        ").compile()\n",
        "\n",
        "t0 = time.time()\n",
        "for i in range(N):\n",
        "  act = jax.random.uniform(\n",
        "      key_act, (num_envs, env.action_size), minval=-1.0, maxval=1.0\n",
        "  )\n",
        "  state = jit_step(state, act)\n",
        "\n",
        "jax.tree_util.tree_map(\n",
        "    lambda x: x.block_until_ready(), state\n",
        ")  # Await device completion\n",
        "dt = time.time() - t0\n",
        "\n",
        "print(\"Madrona MJX: {:d} transitions per second\".format(int(N * num_envs / dt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ab3a297",
      "metadata": {},
      "source": [
        "## Balancing a Cartpole\n",
        "\n",
        "Now, let's train a pixels-to-torque policy to balance a cartpole. In [our implementation]((https://github.com/kevinzakka/mujoco_playground/blob/main/mujoco_playground/_src/dm_control_suite/cartpole.py)), we control the observation dimensionality while encouraging the agent to understand dynamics by setting the observations as grayscale images stacked across sequential timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f3b3ac",
      "metadata": {},
      "source": [
        "#### Visualize the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1286e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "state = jit_reset(jax.random.split(jax.random.PRNGKey(0), num_envs))\n",
        "rollout = [state]\n",
        "\n",
        "f = 0.2\n",
        "for i in range(episode_length):\n",
        "  t = state.data.time\n",
        "  action = []\n",
        "  for j in range(env.action_size):\n",
        "    action.append(0.0)\n",
        "  action = jp.tile(jp.array(action), (num_envs, 1))\n",
        "  state = jit_step(state, action)\n",
        "  rollout.append(state)\n",
        "\n",
        "print(rollout[0].obs[\"pixels/view_0\"].shape)\n",
        "obs = [r.obs[\"pixels/view_0\"][0, ...] for r in rollout]\n",
        "\n",
        "media.show_video(obs, fps=1.0 / env.dt, height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1629dbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(obs[50])\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "686f6663",
      "metadata": {},
      "source": [
        "#### Train\n",
        "Training the policy takes 57s on a RTX4090 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06333cd4",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from mujoco_playground.config import dm_control_suite_params\n",
        "\n",
        "# Load vision-specific PPO configuration tuned for CartpoleBalance\n",
        "ppo_params = dm_control_suite_params.brax_vision_ppo_config(env_name)\n",
        "ppo_params.episode_length = episode_length\n",
        "ppo_params.network_factory = ppo_networks_vision.make_ppo_networks_vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f419702",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "x_data, y_data, y_dataerr = [], [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  clear_output(wait=True)\n",
        "\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics[\"eval/episode_reward\"])\n",
        "  y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
        "\n",
        "  plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n",
        "  plt.ylim([0, 100])\n",
        "  plt.xlabel(\"# environment steps\")\n",
        "  plt.ylabel(\"reward per episode\")\n",
        "  plt.title(f\"y={y_data[-1]:.3f}\")\n",
        "  plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
        "\n",
        "  display(plt.gcf())\n",
        "\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, **dict(ppo_params), progress_fn=progress\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb88915",
      "metadata": {},
      "outputs": [],
      "source": [
        "make_inference_fn, params, metrics = train_fn(environment=env)\n",
        "print(f\"time to jit: {times[1] - times[0]}\")\n",
        "print(f\"time to train: {times[-1] - times[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076c1ca1",
      "metadata": {},
      "source": [
        "#### Visualize the Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd8dec1",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64532268",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(42)\n",
        "rollout = []\n",
        "n_episodes = 1\n",
        "\n",
        "\n",
        "def first_env(x):\n",
        "  return jax.tree.map(\n",
        "      lambda y: y[0] if hasattr(y, \"ndim\") and y.ndim > 0 and y.shape[0] == num_envs else y,\n",
        "      x,\n",
        "  )\n",
        "\n",
        "\n",
        "for _ in range(n_episodes):\n",
        "  reset_rng, rng = jax.random.split(rng)\n",
        "  state = jit_reset(jax.random.split(reset_rng, num_envs))\n",
        "  rollout.append(first_env(state))\n",
        "\n",
        "  for _ in range(episode_length):\n",
        "    policy_rng, rng = jax.random.split(rng)\n",
        "    ctrl, _ = jit_inference_fn(state.obs, jax.random.split(policy_rng, num_envs))\n",
        "    state = jit_step(state, ctrl)\n",
        "    rollout.append(first_env(state))\n",
        "\n",
        "render_every = 1\n",
        "vis_rollout = rollout[::render_every]\n",
        "frames = env.render(vis_rollout, camera=\"fixed\")\n",
        "\n",
        "# Visualize the learned observation stream as valid HxWx3 frames.\n",
        "k = next(iter(vis_rollout[0].obs.keys()))\n",
        "obs_frames = []\n",
        "for s in vis_rollout:\n",
        "  img = np.asarray(s.obs[k])\n",
        "\n",
        "  # Convert to uint8 in [0, 255].\n",
        "  if img.dtype != np.uint8:\n",
        "    img = img.astype(np.float32)\n",
        "    if np.max(img) <= 1.0:\n",
        "      img = img * 255.0\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "  # Ensure mediapy-compatible shape HxWx3.\n",
        "  if img.ndim == 2:\n",
        "    img = np.repeat(img[..., None], 3, axis=-1)\n",
        "  elif img.ndim == 3 and img.shape[-1] == 1:\n",
        "    img = np.repeat(img, 3, axis=-1)\n",
        "  elif img.ndim == 3 and img.shape[-1] == 3:\n",
        "    pass\n",
        "  else:\n",
        "    raise ValueError(f\"Unexpected obs frame shape: {img.shape}\")\n",
        "\n",
        "  obs_frames.append(img)\n",
        "\n",
        "rewards = np.asarray([float(s.reward) for s in rollout])\n",
        "media.show_videos([frames, obs_frames], fps=1.0 / env.dt / render_every)\n",
        "\n",
        "plt.plot(np.convolve(rewards, np.ones(10) / 10, mode=\"valid\"))\n",
        "plt.xlabel(\"time step\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eddb3fc3",
      "metadata": {},
      "source": [
        "ðŸ™Œ See you in Part 2 of pixels-based training with Madrona-MJX, where we'll cover how to use visual domain randomization to transfer policies to the real world!"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
